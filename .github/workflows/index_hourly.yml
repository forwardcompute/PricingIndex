name: Forward Compute Index â€” Hourly Build & Deploy

on:
  workflow_dispatch:
  schedule:
    - cron: "5 * * * *"   # every hour at minute 5

permissions:
  contents: write
  pages: write
  id-token: write

env:
  PYTHON_VERSION: "3.12"
  COMMIT_DATA: "false"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) SCRAPE: run one job per provider in parallel every hour
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
jobs:
  scrape:
    name: Scrape ${{ matrix.provider_name }}
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        include:
          # Map UI name -> internal id your scraper expects
          - provider_name: AWS              ; provider_id: aws
          - provider_name: GCP              ; provider_id: gcp
          - provider_name: Azure            ; provider_id: azure
          - provider_name: CoreWeave        ; provider_id: coreweave
          - provider_name: Lambda Labs      ; provider_id: lambda_labs
          - provider_name: Crusoe           ; provider_id: crusoe
          - provider_name: RunPod           ; provider_id: runpod
          - provider_name: Paperspace       ; provider_id: paperspace
          - provider_name: FluidStack       ; provider_id: fluidstack
          - provider_name: Nebius           ; provider_id: nebius
          - provider_name: Together.ai      ; provider_id: together
          - provider_name: Replicate        ; provider_id: replicate
          - provider_name: Jarvislabs       ; provider_id: jarvislabs
          - provider_name: Vast.ai          ; provider_id: vast_ai
          - provider_name: OVHcloud         ; provider_id: ovhcloud
          - provider_name: Hyperstack       ; provider_id: hyperstack
          - provider_name: TensorDock       ; provider_id: tensordock
          - provider_name: Voltage Park     ; provider_id: voltage_park
          - provider_name: Scaleway         ; provider_id: scaleway
          - provider_name: Genesis Cloud    ; provider_id: genesis_cloud
          - provider_name: SF Compute       ; provider_id: sf_compute

    concurrency:
      # cancel older in-progress run for the *same provider* only
      group: forward-compute-index-${{ matrix.provider_id }}
      cancel-in-progress: true

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name:  Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name:  Install dependencies
        run: |
          set -e
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps

      - name:  Scrape ${{ matrix.provider_name }}
        env:
          HCPI_DATABASE_URL: ${{ secrets.HCPI_DATABASE_URL }}
          LAMBDA_API_KEY:    ${{ secrets.LAMBDA_API_KEY }}
          RUNPOD_API_KEY:    ${{ secrets.RUNPOD_API_KEY }}
        run: |
          set -e
          echo " Scraping provider: ${{ matrix.provider_id }}"
          # Prefer your provider-targeted entrypoint if it exists:
          # e.g. python providers/scrape.py --provider "${{ matrix.provider_id }}"
          # Fallback to your master runner with a provider flag:
          if python - <<'PY'
import sys, importlib.util
print(importlib.util.find_spec("providers"))
PY
          then
            echo "providers/ package detected"
          fi

          # Try a provider-specific scrape first; fallback to master_runner
          ( python scrape_provider.py --provider "${{ matrix.provider_id }}" \
            || python master_runner.py --provider "${{ matrix.provider_id }}" --cron --db "${HCPI_DATABASE_URL:-sqlite:///hcpi.db}" \
            || echo " Provider scrape failed for ${{ matrix.provider_id }} (continuing)" )

          # Ensure output folder exists even if provider produced none
          mkdir -p out/${{ matrix.provider_id }}

          # If your scraper writes to common data paths, copy provider-sliced files here too
          # (adjust these patterns to what your code emits)
          cp -v data/derived/*${{ matrix.provider_id }}*.csv out/${{ matrix.provider_id }}/ 2>/dev/null || true
          cp -v data/raw/*${{ matrix.provider_id }}*.json out/${{ matrix.provider_id }}/ 2>/dev/null || true
          # Also copy the latest â€œflatâ€ CSV rows for this provider if present
          awk -F, 'NR==1 || tolower($0) ~ /${{ matrix.provider_id }}/' data/derived/provider_scores_latest.csv \
            > out/${{ matrix.provider_id }}/provider_scores_${{ matrix.provider_id }}.csv 2>/dev/null || true

          echo " Provider artifact tree:"
          find out/${{ matrix.provider_id }} -type f -maxdepth 1 -print || true

      - name: Upload provider artifact
        uses: actions/upload-artifact@v4
        with:
          name: provider-${{ matrix.provider_id }}
          path: out/${{ matrix.provider_id }}
          if-no-files-found: warn
          retention-days: 3

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 2) AGGREGATE: merge provider artifacts â†’ regenerate CSV/HCPI â†’ build site
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  aggregate-and-build:
    name: Aggregate & Build
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: scrape
    concurrency:
      group: forward-compute-index-aggregate
      cancel-in-progress: true

    steps:
      - name:  Checkout
        uses: actions/checkout@v4

      - name:  Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - name: Install dependencies
        run: |
          set -e
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps

      - name:  Download all provider artifacts
        uses: actions/download-artifact@v4
        with:
          path: provider_artifacts

      - name: Merge provider CSVs â†’ provider_scores_latest.csv
        run: |
          set -e
          mkdir -p data/derived data/history hcpi
          python - <<'PY'
import csv, glob, os, sys
from datetime import datetime

out_dir = "data/derived"
os.makedirs(out_dir, exist_ok=True)
merged_path = os.path.join(out_dir, "provider_scores_latest.csv")

# Collect all provider CSVs from artifacts
paths = glob.glob("provider_artifacts/provider-*/*.csv")
rows = []
header = None
for p in paths:
    with open(p, newline='', encoding='utf-8') as f:
        r = csv.reader(f)
        h = next(r, None)
        if h is None: 
            continue
        if header is None:
            header = h
        # If headers mismatch, align by name as best effort
        for row in r:
            if not any(row):
                continue
            rows.append(row)

if header is None:
    # fallback: if repo already has a latest csv, keep it; else create minimal header
    if os.path.exists(merged_path):
        print(" No artifacts found; keeping existing provider_scores_latest.csv")
        sys.exit(0)
    header = ["provider","region","effective_price_usd_per_gpu_hr"]
    rows = []

with open(merged_path, "w", newline='', encoding='utf-8') as f:
    w = csv.writer(f)
    w.writerow(header)
    w.writerows(rows)

# Append simple history snapshot
hist_dir = "data/history"
os.makedirs(hist_dir, exist_ok=True)
stamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
hist_path = os.path.join(hist_dir, f"provider_scores_{stamp}.csv")
with open(hist_path, "w", newline='', encoding='utf-8') as f:
    r = csv.reader(open(merged_path, newline='', encoding='utf-8'))
    w = csv.writer(f); 
    for row in r: w.writerow(row)

print(f" Wrote {merged_path} ({len(rows)} rows)")
print(f" Snapshot: {hist_path}")
PY

      - name:  Recompute HCPI / dashboard JSON
        env:
          HCPI_DATABASE_URL: ${{ secrets.HCPI_DATABASE_URL }}
        run: |
          set -e
          echo "ðŸ” Running index calculatorsâ€¦"
          # Your updated scripts; adjust paths as needed:
          python index_calculator.py || true
          # If you have a dedicated export:
          if [ -f index_calculator.py ]; then
            python - <<'PY'
import json, os
from datetime import datetime
# Optional: if index_calculator.py already exports, skip this
if os.path.exists("hcpi/hcpi_dashboard.json"):
    print("hcpi_dashboard.json already exists.")
else:
    # Minimal fallback dashboard JSON
    os.makedirs("hcpi", exist_ok=True)
    dash = {
        "timestamp": datetime.utcnow().isoformat()+"Z",
        "us_index": None,
        "regions": {},
        "categories": {}
    }
    json.dump(dash, open("hcpi/hcpi_dashboard.json","w"), indent=2)
    print("Wrote hcpi/hcpi_dashboard.json (fallback)")
PY
          fi
          # Ensure latest files exist (HCPI + full JSON)
          ls -l hcpi || true

      - name: ðŸ”Ž Verify merged data
        run: |
          echo "==== provider_scores_latest.csv ===="
          wc -l data/derived/provider_scores_latest.csv || true
          head -5 data/derived/provider_scores_latest.csv || true
          tail -5 data/derived/provider_scores_latest.csv || true
          echo "==== hcpi folder ===="
          ls -lah hcpi || true

      - name: ðŸ’¾ Commit generated data (if enabled)
        if: env.COMMIT_DATA == 'true'
        uses: EndBug/add-and-commit@v9
        with:
          add: |
            data/derived/*.csv
            data/history/*.csv
            hcpi/*.json
            hcpi/*history*.csv
          message: "ci: hourly provider scrape + index [skip ci]"
          author_name: github-actions
          author_email: actions@github.com

      - name: ðŸ—ï¸ Prepare static site
        run: |
          set -e
          mkdir -p public/data/derived public/data/history public/hcpi
          
          echo "==== Copying HTML ===="
          if [ -f docs/index.html ]; then
            cp -v docs/index.html public/index.html
          elif [ -f index.html ]; then
            cp -v index.html public/index.html
          else
            echo '<!doctype html><meta charset="utf-8"><title>Forward Compute Index</title><body style="font:14px system-ui;padding:24px;color:#e7edf3;background:#0f1216"><h1>Forward Compute Index</h1><p>No dashboard found. Add <code>docs/index.html</code> to the repo.</p></body>' > public/index.html
          fi
          
          echo "==== Copying data files ===="
          cp -v data/derived/*.csv public/data/derived/ 2>/dev/null || true
          cp -v data/history/*.csv  public/data/history/  2>/dev/null || true
          cp -v hcpi/*.json         public/hcpi/          2>/dev/null || true
          cp -v hcpi/*history*.csv  public/hcpi/          2>/dev/null || true

          echo "==== Verify public/data/derived/provider_scores_latest.csv ===="
          if [ -f public/data/derived/provider_scores_latest.csv ]; then
            wc -l public/data/derived/provider_scores_latest.csv
            head -3 public/data/derived/provider_scores_latest.csv
            tail -3 public/data/derived/provider_scores_latest.csv
          else
            echo " ERROR: provider_scores_latest.csv missing after copy!"
            exit 1
          fi

          echo "==== public/ tree ===="
          find public -type f -print

      - name: ðŸ“¤ Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # 3) DEPLOY PAGES
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  deploy:
    name: Deploy to GitHub Pages
    needs: aggregate-and-build
    runs-on: ubuntu-latest
    timeout-minutes: 10
    environment:
      name: github-pages
    permissions:
      pages: write
      id-token: write
    steps:
      - name:  Deploy
        id: deployment
        uses: actions/deploy-pages@v4



